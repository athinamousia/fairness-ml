{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#overview","title":"Overview","text":"<p>Machine learning algorithms increasingly drive critical decisions in finance, healthcare, criminal justice, and education. While these systems offer powerful predictive capabilities, they also raise fundamental concerns about fairness and bias that must be addressed for responsible AI deployment.</p> <p>Fairness in machine learning means that algorithms provide equitable outcomes across demographic groups without systematically disadvantaging certain populations.   </p> <p>Bias enters ML systems through two primary pathways: data bias (skewed or unrepresentative training data reflecting historical inequities, incomplete diversity, or measurement errors) and model bias (algorithmic choices and feature selections that amplify existing biases).   </p> <p>Sensitive attributes like race, gender, or socioeconomic status can become entangled with other features, causing models to discriminate even when these attributes aren't explicitly used.  </p> <p>This project addresses algorithmic fairness using predictive models to identify students' droppout and academic success.This analysis investigates how bias appears in student success prediction and evaluates 7 fairness intervention techniques across pre-processing, in-processing, and post-processing stages to mitigate discriminatory patterns while maintaining predictive accuracy.</p>"},{"location":"#dataset","title":"Dataset","text":"<p>The analysis uses the Predict Students' Dropout and Academic Success dataset from the UCI Machine Learning Repository, containing real-world academic retention data with demographic, socioeconomic, and academic performance features.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized into four main sections:</p> <ol> <li>Exploratory Data Analysis: Statistical analysis of data distributions, correlations, and patterns that may contribute to model bias before training begins.</li> <li>Bias Detection: Pre-modeling and post-modeling bias identification</li> <li>Bias Mitigation: Detailed evaluation of seven fairness intervention methods</li> <li>Explainability: SHAP (SHapley Additive exPlanations) analysis to understand feature contributions and prediction rationale.</li> </ol> <p>Navigate through the sections using the sidebar to explore the complete analysis, methodology, results, and interpretations.</p>"},{"location":"#framework-and-tools","title":"Framework and Tools","text":"Technology Purpose AIF360 IBM's AI Fairness 360 toolkit for bias detection and mitigation XGBoost Gradient boosting baseline classifier TensorFlow Deep learning framework for Adversarial Debiasing SHAP Model explainability and interpretability scikit-learn Machine learning utilities and evaluation metrics <p>Author: Athina Mousia UoM - Business Analytics and Data Science Master's Thesis Repository: github.com/athinamousia/fairness-ml</p>"},{"location":"bias_detection/bias_detection/","title":"Bias Detection Overview","text":"<p>Bias detection involves identifying and analyzing systematic unfairness in data, algorithms, or decision-making systems. Bias occurs when certain groups receive disproportionate treatment, leading to inequitable outcomes. Detecting and addressing bias is essential for ensuring fairness, transparency, and accountability in machine learning systems.</p> <p>Sources of bias include imbalanced data representation, historical discrimination encoded in training data, and algorithmic design choices that amplify existing inequalities. Systematic detection helps uncover these issues before they manifest in real-world applications.</p>"},{"location":"bias_detection/bias_detection/#bias-detection-approaches","title":"Bias Detection Approaches","text":"<p>This project employs two complementary approaches to bias detection:</p> Type Description Methods Pre-modeling Bias Detection Conducted before model training to assess data-level fairness Statistical analysis, correlation metrics, distribution analysis across demographic groups Post-modeling Bias Detection Conducted after model training to evaluate prediction-level fairness Fairness metrics (disparate impact, equal opportunity, demographic parity), performance disparities across groups <p>Both approaches are critical for comprehensive fairness assessment. Pre-modeling detection prevents bias from entering the model, while post-modeling detection evaluates whether the trained model produces equitable outcomes. Together, they provide a systematic framework for identifying unfairness throughout the machine learning pipeline.</p>"},{"location":"bias_detection/post_modeling_bias/","title":"Post-Modeling Bias Detection","text":"<p>Post-modeling bias detection evaluates whether a trained model produces fair predictions across different demographic groups. This stage assesses model outputs to identify disparate treatment or impact that may disadvantage protected populations.</p>"},{"location":"bias_detection/post_modeling_bias/#fairness-metrics","title":"Fairness Metrics","text":"<p>Four key metrics quantify different dimensions of algorithmic fairness:</p> <ul> <li> <p>Statistical Parity Difference (SPD): Measures the difference in positive prediction rates between privileged and unprivileged groups. A value of zero indicates equal treatment; deviations from zero signal potential bias.</p> </li> <li> <p>Disparate Impact Ratio (DIR): Compares the ratio of positive outcomes between unprivileged and privileged groups. A value of 1.0 indicates parity. Values below 0.8 or above 1.25 are commonly considered evidence of disparate impact.</p> </li> <li> <p>Equal Opportunity Difference (EOD): Quantifies the difference in true positive rates (recall) between groups. A value near zero ensures both groups have equal probability of correctly receiving positive predictions when they should.</p> </li> <li> <p>Average Odds Difference (AOD): Aggregates differences in both false positive and true positive rates across groups. A value near zero indicates balanced performance, minimizing both missed opportunities and false alarms for all groups.</p> </li> </ul> Metric Equation Meaning of Symbols Statistical Parity Difference (SPD) \\(\\text{SPD} = P(\\hat{Y} = 1 \\mid A = 1) - P(\\hat{Y} = 1 \\mid A = 0)\\) \\(A\\): protected attribute (1 = privileged, 0 = unprivileged); \\(\\hat{Y}\\): model prediction Disparate Impact Ratio (DIR) \\(\\text{DIR} = \\frac{P(\\hat{Y} = 1 \\mid A = 0)}{P(\\hat{Y} = 1 \\mid A = 1)}\\) Ratio of positive outcomes for unprivileged vs privileged group Equal Opportunity Difference (EOD) \\(\\text{EOD} = \\text{TPR}_{A=1} - \\text{TPR}_{A=0}\\) \\(\\text{TPR}\\): True Positive Rate = \\(P(\\hat{Y} = 1 \\mid Y = 1)\\) Average Odds Difference (AOD) \\(\\text{AOD} = \\frac{[(\\text{TPR}_{A=1} - \\text{TPR}_{A=0}) + (\\text{FPR}_{A=1} - \\text{FPR}_{A=0})]}{2}\\) \\(\\text{FPR}\\): False Positive Rate = \\(P(\\hat{Y} = 1 \\mid Y = 0)\\) <p>Key Findings:</p> Protected Attribute Statistical Parity Difference Disparate Impact Ratio Equal Opportunity Difference Average Odds Difference Marital status 0.2382 1.5461 0.1472 0.1918 Application mode 0.6556 0.0 0.846 0.6056 Application order 0.1805 1.3034 0.101 0.1406 Course 0.0304 1.0486 0.0617 -0.0231 Daytime/evening attendance -0.1814 0.7303 -0.2262 -0.1534 Previous qualification 0.6593 0.0 0.8473 0.6084 Previous qualification (grade) - - 0.8447 0.1041 Nationality - - 0.8447 0.1041 Mother's qualification -0.0131 0.9803 0.0603 -0.0029 Father's qualification 0.1558 1.3116 0.2763 0.105 Mother's occupation 0.1579 1.3157 0.1333 0.1068 Father's occupation 0.0788 1.1365 0.0331 0.1002 Admission grade - - 0.8447 0.1041 Displaced -0.187 0.7459 -0.0901 -0.1409 Educational special needs -0.3488 0.6512 -0.1567 -0.3981 Debtor 0.4866 3.135 0.2388 0.2717 Gender 0.39 1.9886 0.3081 0.2953 Scholarship holder -0.3992 0.5809 -0.2127 -0.3436 Age at enrollment -0.3346 0.6308 -0.1441 -0.3251 International -0.0872 0.8821 -0.1581 -0.1258 Unemployment rate -0.0576 0.9185 0.0155 -0.0157 Inflation rate -0.0575 0.9186 -0.0757 -0.0759 GDP 0.003 1.0046 0.0154 -0.0044 <p>The baseline XGBoost model identifies significant fairness considerations across multiple protected attributes (bold values indicate concerning bias):</p> <p>Critical Fairness Issues:</p> <ul> <li> <p>Application mode (SPD=0.66, EOD=0.85, AOD=0.61): The most severe bias detected. Students from certain application pathways face dramatically different prediction outcomes, with some groups systematically disadvantaged in graduation predictions.</p> </li> <li> <p>Previous qualification (SPD=0.66, EOD=0.85, AOD=0.61): Strong disparities based on prior educational background, potentially perpetuating existing educational inequalities.</p> </li> <li> <p>Debtor (SPD=0.49, DIR=3.14): Students in debt are substantially more likely to receive negative predictions. The DIR of 3.14 indicates debtors receive positive outcomes at three times the rate of non-debtors, revealing extreme model bias tied to financial status.</p> </li> <li> <p>Gender (SPD=0.39, DIR=1.99): Male students receive positive predictions at nearly twice the rate of female students, representing significant gender-based discrimination in model outputs.</p> </li> <li> <p>Scholarship holder (SPD=-0.40, AOD=-0.34): Scholarship recipients face lower positive prediction rates, disadvantaging students with financial support.</p> </li> <li> <p>Age at enrollment (SPD=-0.33, AOD=-0.33): Older students receive systematically worse predictions, discriminating against non-traditional students.</p> </li> <li> <p>Educational special needs (SPD=-0.35, AOD=-0.40): Students with special needs face substantial bias, indicating the model may unfairly predict lower graduation likelihood for this vulnerable population.</p> </li> </ul>"},{"location":"bias_detection/post_modeling_bias/#model-performance-evaluation","title":"Model Performance Evaluation","text":"<p>The baseline XGBoost model demonstrates strong predictive performance while revealing important insights about prediction patterns:</p> <p>Confusion Matrix Analysis:</p> Prediction Type Count Interpretation True Negatives 275 Correctly predicted dropouts (target=0), showing good ability to identify at-risk students True Positives 555 Correctly predicted graduates (target=1), demonstrating strong performance on the majority class False Positives 157 Incorrectly predicted graduation when students actually dropped out\u2014missed intervention opportunities False Negatives 102 Incorrectly predicted dropout when students actually graduated\u2014potentially denying support or opportunities <p>Performance Metrics:</p> Metric Value Interpretation Balanced Accuracy 0.74 Accounts for class imbalance by averaging recall across both classes, indicating reasonably good performance on both graduates and dropouts F1-Score 0.81 Harmonic mean of precision and recall shows strong overall predictive capability, slightly favoring the majority (graduate) class ROC-AUC 0.83 Strong discriminative ability to distinguish between graduates and dropouts across various decision thresholds <p>The model achieves solid predictive performance (ROC-AUC=0.83) but the confusion matrix reveals an important pattern: the model is more likely to incorrectly predict graduation (157 false positives) than incorrectly predict dropout (102 false negatives). This asymmetry, combined with the significant fairness considerations identified above, suggests the model's errors may not be randomly distributed across demographic groups. </p>"},{"location":"bias_detection/pre_modeling_bias/","title":"Pre-Modeling Bias Detection","text":"<p>Pre-modeling bias detection examines the dataset before model training to identify potential sources of unfairness. Early detection enables proactive mitigation strategies, improving model fairness, reliability, and interpretability.</p> <p>This analysis employs statistical techniques to uncover relationships between features and sensitive attributes that may lead to discriminatory outcomes:</p> <ul> <li>Correlation analysis - Quantifies relationships between variables</li> <li>Kernel Density Estimation (KDE) - Visualizes distributional differences across groups</li> <li>Hypothesis testing - Assesses statistical significance of observed patterns</li> </ul>"},{"location":"bias_detection/pre_modeling_bias/#spearman-correlation-analysis","title":"Spearman Correlation Analysis","text":"<p>The Spearman correlation coefficient (\u03c1) measures monotonic relationships between variables using rank-based comparisons rather than raw values. This approach is robust to non-linear relationships, non-normal distributions, and outliers, making it ideal for fairness analysis.</p> \u03c1 value Interpretation +1 Perfect positive monotonic relationship 0 No monotonic relationship -1 Perfect negative monotonic relationship <p>Statistical Significance:</p> <ul> <li>p &lt; 0.05: Statistically significant correlation (unlikely due to chance)</li> <li>p \u2265 0.05: Not statistically significant (may be due to random variation)</li> </ul> <p></p> <p>The Spearman correlation analysis with the target variable (1 = graduation, 0 = dropout) reveals key predictive relationships. Statistically significant correlations (marked with *) indicate features most associated with student outcomes:</p> <p>Key Findings:</p> <ul> <li>Scholarship holder (\u03c1 = 0.31)*: Strongest positive predictor. Scholarship recipients have significantly higher graduation rates.</li> <li>Age at enrollment (\u03c1 = -0.32)*: Strongest negative predictor. Older students face substantially higher dropout risk, with non-traditional students experiencing greater challenges</li> <li>Debtor (\u03c1 = -0.27)*: Being in debt strongly correlates with dropout, highlighting financial hardship as a critical barrier to completion.</li> <li>Gender (\u03c1 = -0.25)*: Male studentsshow higher dropout rates compared to female students, revealing gender-based disparities in academic retention.</li> <li>Application mode (\u03c1 = -0.24)*: Enrollment pathway influences retention, with certain admission routes associated with higher dropout risk</li> <li>Admission grade (\u03c1 = 0.13) and Previous qualification (grade) (\u03c1 = 0.12): Academic preparation positively predicts graduation</li> </ul> <p>Notably, economic indicators (Unemployment rate, Inflation rate, GDP), parental factors, and demographic variables (Nationality, International) show minimal correlation, suggesting limited direct impact on individual student retention.</p> <p>The strong correlations with Age, Debtor, Scholarship holder and Gender represent potential bias sources, as models trained on these features may systematically disadvantage certain demographic groups. </p>"},{"location":"bias_detection/pre_modeling_bias/#kernel-density-estimation-kde","title":"Kernel Density Estimation (KDE)","text":"<p>Kernel Density Estimation visualizes the probability distribution of variables across different demographic groups. By comparing distribution curves between groups (e.g., graduates vs. dropouts), KDE plots reveal whether certain features exhibit systematic differences.</p>"},{"location":"bias_detection/pre_modeling_bias/#interpreting-kde-plots","title":"Interpreting KDE Plots","text":"<ul> <li>Overlapping distributions: Feature values are similar across groups, suggesting lower bias risk</li> <li>Separated distributions: Distinct patterns between groups indicate the feature may contribute to differential treatment</li> </ul> <p>Significant distributional differences, particularly for features correlated with sensitive attributes, signal potential fairness concerns that need further investigation.</p> <p></p> <p>The KDE distributions compare feature distributions between graduates and dropouts (1 = graduation, 0 = dropout), revealing key patterns with fairness implications:</p> <p>Key Findings:</p> <ul> <li>Debtor: Clear distributional separation shows non-debtors (0) concentrate heavily in the graduation group, while debtors (1) are more prevalent among dropouts, confirming financial hardship as a critical barrier.</li> <li>Gender: Male students (1) show higher concentration in the dropout distribution compared to females (0), aligning with the negative correlation observed.</li> <li>Scholarship holder: Scholarship recipients (1) demonstrate stronger concentration in the graduation distribution, while non-recipients (0) are more prevalent among dropouts.</li> <li>Age at enrollment: Graduates tend to be younger at enrollment, with the dropout distribution showing a rightward shift toward older ages, consistent with the negative correlation.</li> <li>Application mode and Course: Different enrollment pathways and course selections show distinct distributional patterns between graduates and dropouts, indicating these institutional factors significantly influence retention.</li> <li>Parental background (qualification/occupation): Distributions reveal socioeconomic stratification, with students from higher socioeconomic backgrounds showing greater concentration in the graduation group.</li> </ul> <p>These distributional differences, particularly for Debtor, Gender, Scholarship holder, and Age, represent potential bias sources that models may learn and perpetuate, warranting careful consideration in fairness-aware modeling.</p>"},{"location":"bias_mitigation/bias_mitigation/","title":"Bias Mitigation","text":"<p>Bias mitigation techniques address fairness violations by reducing discriminatory patterns that machine learning models learn from biased training data. Without intervention, models can perpetuate and amplify existing societal inequalities, leading to systematically unfair outcomes for protected demographic groups.</p>"},{"location":"bias_mitigation/bias_mitigation/#mitigation-strategies","title":"Mitigation Strategies","text":"<p>Bias mitigation approaches been used in the next sections are summarized below:</p> Category Timing Purpose Model Techniques Pre-processing Before training Transform data to remove bias while preserving predictive information Reweighing, Learning Fair Representations In-processing During training Incorporate fairness constraints directly into model optimization Adversarial Debiasing, Exponentiated Gradient Reduction Post-processing After training Adjust model outputs to satisfy fairness criteria without retraining Calibrated Equalized Odds, Reject Option Classification <p>This project implements bias mitigation techniques using AI Fairness 360 (AIF360), an open-source toolkit developed by IBM Research. AIF360 provides comprehensive algorithms for detecting and mitigating bias across the machine learning lifecycle, along with standardized fairness metrics for evaluation.</p> <p>The following sections present each mitigation approach, demonstrate its application to the student retention dataset, evaluate performance using fairness metrics, and compare results against the baseline model to quantify improvements in both fairness and predictive accuracy.</p>"},{"location":"bias_mitigation/in_processing_models/","title":"In-Processing Bias Mitigation","text":"<p>In-processing techniques incorporate fairness constraints directly into the model training process. By embedding fairness objectives into the learning algorithm itself, these methods simultaneously optimize for both predictive accuracy and equitable treatment across demographic groups, without requiring data preprocessing or post-hoc adjustments.</p>"},{"location":"bias_mitigation/in_processing_models/#adversarial-debiasing","title":"Adversarial Debiasing","text":"<p>B. H. Zhang, B. Lemoine, and M. Mitchell, \u201cMitigating Unwanted Biases with  Adversarial Learning,\u201d AAAI/ACM Conference on Artificial Intelligence, Ethics, and  Society, 2018. Adversarial Debiasing employs an adversarial training framework to learn fair representations during model optimization. The technique uses two competing neural networks trained simultaneously through a minimax game:</p> <ul> <li>Predictor: Learns to predict the target outcome while minimizing predictive loss</li> <li>Adversary: Attempts to predict protected attributes from the predictor's internal representations</li> </ul> <p>The predictor is optimized to achieve two competing objectives: maximize prediction accuracy on the target variable while simultaneously minimizing the adversary's ability to infer protected attributes from its representations. This adversarial objective forces the predictor to learn features that are informative for the task but invariant to sensitive attributes.</p> <p>This process is repeated in several steps: 1. Train the Predictor to predict the target variable. 2. Train the Adversary to predict the protected attribute from the Predictor\u2019s output. 3. Update both models so the Predictor becomes less informative about the protected attribute, and the Adversary becomes less able to predict it. 4. Repeat until the Predictor is accurate and fair.</p> <p>This approach is flexible, supporting multiple fairness criteria (demographic parity, equalized odds) and applicable to various model architectures and data types. The key advantage is achieving fairness without requiring dataset modifications or architectural constraints beyond the adversarial training framework.</p> <p>Results:</p> <p></p> <p>Adversarial Debiasing achieves moderate fairness improvements while maintaining reasonable predictive performance:</p> <p>Performance Metrics:</p> Performance Metric Baseline Adversarial Debiasing Balanced Accuracy 0.74 0.70 F1-Score 0.81 0.77 ROC-AUC 0.83 0.76 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline Adversarial Debiasing Improvement Debtor Statistical Parity Difference 0.49 0.32 \u2713 Disparate Impact Ratio 3.14 1.94 \u2713 Equal Opportunity Difference 0.24 -0.02 \u2713 Average Odds Difference 0.27 0.09 \u2713 Gender Statistical Parity Difference 0.39 0.23 \u2713 Disparate Impact Ratio 1.99 1.47 \u2713 Equal Opportunity Difference 0.31 0.14 \u2713 Average Odds Difference 0.30 0.14 \u2713 Scholarship holder Statistical Parity Difference -0.40 -0.46 \u2717 Disparate Impact Ratio 0.58 0.53 \u2717 Equal Opportunity Difference -0.21 -0.31 \u2717 Average Odds Difference -0.34 -0.43 \u2717 <p>Adversarial Debiasing achieves mixed results, improving fairness for Debtor and Gender while degrading Scholarship holder metrics. </p> <p>This degradation occurs because the adversarial training was prioritized to address fairness violations in the Debtor and Gender attributes. Including more protected attributes in adversarial training increases model complexity and creates competing optimization objectives, leading to performance degradation.</p> <p>The 8% drop in ROC-AUC (0.76 vs 0.83 baseline) represents a moderate accuracy-fairness tradeoff. </p> <p>These results highlight the challenge of adversarial debiasing in multi-attribute scenarios, where increasing the number of protected attributes amplifies the difficulty of achieving uniform fairness improvements across all groups.</p>"},{"location":"bias_mitigation/in_processing_models/#exponentiated-gradient-reduction-egr","title":"Exponentiated Gradient Reduction (EGR)","text":"<p>A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach, \u201cA Reductions Approach to Fair Classification,\u201d International Conference on Machine Learning, 2018. </p> <p>Exponentiated Gradient Reduction is an in-processing technique that formulates fair classification as a constrained optimization problem. Rather than modifying the learning algorithm internally, EGR treats the base classifier as a black box and applies fairness constraints through a reduction framework.</p> <p>The approach decomposes the fairness-constrained learning problem into a sequence of cost-sensitive classification tasks. At each iteration, the algorithm:</p> <ul> <li>Evaluates current fairness violations across protected groups</li> <li>Constructs cost-sensitive weights that penalize predictions contributing to unfairness</li> <li>Trains a new classifier on the reweighted data</li> <li>Combines classifiers using learned mixture weights</li> </ul> <p>EGR supports multiple fairness criteria through linear constraints:</p> <ul> <li>Demographic Parity: \\(P(\\hat{Y}=1 | A=0) = P(\\hat{Y}=1 | A=1)\\)</li> <li>Equalized Odds: \\(P(\\hat{Y}=1 | Y=y, A=0) = P(\\hat{Y}=1 | Y=y, A=1)\\) for all \\(y\\)</li> </ul> <p>The algorithm uses exponentiated gradient updates to iteratively adjust the mixture of classifiers until fairness constraints are satisfied within a specified tolerance. The final model is a randomized ensemble that balances accuracy and fairness optimally.</p> <p>Results:</p> <p></p> <p>Exponentiated Gradient Reduction achieves strong fairness improvements with significant performance impact:</p> <p>Performance Metrics:</p> Performance Metric Baseline EGR Balanced Accuracy 0.74 0.67 F1-Score 0.81 0.78 ROC-AUC 0.83 0.72 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline EGR Improvement Debtor Statistical Parity Difference 0.49 0.18 \u2713 Disparate Impact Ratio 3.14 1.31 \u2713 Equal Opportunity Difference 0.24 -0.02 \u2713 Average Odds Difference 0.27 0.01 \u2713 Gender Statistical Parity Difference 0.39 0.14 \u2713 Disparate Impact Ratio 1.99 1.23 \u2713 Equal Opportunity Difference 0.31 0.01 \u2713 Average Odds Difference 0.30 0.08 \u2713 Scholarship holder Statistical Parity Difference -0.40 -0.17 \u2713 Disparate Impact Ratio 0.58 0.80 \u2713 Equal Opportunity Difference -0.21 -0.05 \u2713 Average Odds Difference -0.34 -0.09 \u2713 <p>Exponentiated Gradient Reduction delivers exceptional fairness improvements across all protected attributes and metrics. </p> <p>However, these fairness gains come at the cost of substantial performance degradation. The method experiences a 13% ROC-AUC decrease, a 9% drop in Balanced Accuracy, and a 4% reduction in F1-Score. </p> <p>Unlike Adversarial Debiasing's mixed results, EGR improves all 12 fairness metrics uniformly, demonstrating its effectiveness as a comprehensive solution for multi-attribute fairness in student retention prediction, albeit with significant predictive performance sacrifice.</p>"},{"location":"bias_mitigation/post_processing_models/","title":"Post-Processing Bias Mitigation","text":"<p>Post-processing techniques adjust model predictions after training to satisfy fairness constraints. By modifying outputs rather than training data or algorithms, these methods enable fairness corrections without retraining, making them particularly useful for deployed models or when training data cannot be modified.</p>"},{"location":"bias_mitigation/post_processing_models/#calibrated-equalized-odds","title":"Calibrated Equalized Odds","text":"<p>G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, \u201cOn Fairness and Calibration,\u201d Conference on Neural Information Processing Systems, 2017</p> <p>Calibrated Equalized Odds is a post-processing technique that adjusts classifier decision thresholds to achieve equalized odds\u2014ensuring equal true positive rates and false positive rates across protected groups. The method operates on the predicted probabilities or scores from any trained classifier, applying group-specific threshold adjustments to satisfy fairness constraints.</p> <p>The algorithm optimizes a linear transformation of predicted probabilities for each protected group:</p> <ul> <li>Computes group-specific mixing rates that determine how to blend base predictions with random decisions</li> <li>Finds optimal thresholds that minimize classification error subject to equalized odds constraints</li> <li>Applies calibrated decision rules that map predicted probabilities to final binary predictions</li> </ul> <p>The method enforces equalized odds by ensuring:</p> <p>\\(P(\\hat{Y}=1 | Y=y, A=0) = P(\\hat{Y}=1 | Y=y, A=1)\\) for \\(y \\in \\{0, 1\\}\\)</p> <p>This guarantees equal treatment across demographic groups for both positive and negative true labels. The calibration process finds the fairest decision thresholds while maintaining the best possible accuracy within the constraint space.</p> <ul> <li>Model-agnostic: Works with any classifier that outputs probability scores</li> <li>No retraining required: Adjusts predictions from existing models</li> <li>Theoretical guarantees: Provably achieves optimal accuracy under equalized odds constraints</li> </ul> <p>Results:</p> <p></p> <p>Calibrated Equalized Odds achieves mixed fairness improvements with significant performance degradation:</p> <p>Performance Metrics:</p> Performance Metric Baseline Calibrated Equalized Odds Balanced Accuracy 0.74 0.60 F1-Score 0.81 0.79 ROC-AUC 0.83 0.61 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline Calibrated Equalized Odds Improvement Debtor Statistical Parity Difference 0.49 0.74 \u2717 Disparate Impact Ratio 3.14 3.89 \u2717 Equal Opportunity Difference 0.24 0.35 \u2717 Average Odds Difference 0.27 0.59 \u2717 Gender Statistical Parity Difference 0.39 0.09 \u2713 Disparate Impact Ratio 1.99 1.10 \u2713 Equal Opportunity Difference 0.31 0.02 \u2713 Average Odds Difference 0.30 0.05 \u2713 Scholarship holder Statistical Parity Difference -0.40 -0.09 \u2713 Disparate Impact Ratio 0.58 0.91 \u2713 Equal Opportunity Difference -0.21 -0.00 \u2713 Average Odds Difference -0.34 -0.07 \u2713 <p>Calibrated Equalized Odds demonstrates highly inconsistent results across protected attributes. While Gender and Scholarship holder achieve substantial fairness improvements, all four Debtor metrics worsen dramatically.</p> <p>The performance impact is equally concerning: ROC-AUC drops 27%, and Balanced Accuracy decreases 19%, representing the worst predictive performance among all mitigation methods tested. </p> <p>This suggests the threshold calibration process overcompensated for certain groups while amplifying bias for others. </p>"},{"location":"bias_mitigation/post_processing_models/#reject-option-classification","title":"Reject Option Classification","text":"<p>F. Kamiran, A. Karim, and X. Zhang, \u201cDecision Theory for Discrimination-Aware Classification,\u201d IEEE International Conference on Data Mining, 2012.</p> <p>Reject Option Classification is a post-processing technique that adjusts predictions within a \"critical region\" near the decision boundary where the classifier is most uncertain. Rather than applying uniform threshold shifts, this method selectively modifies predictions for instances where small probability changes can significantly improve fairness without sacrificing much accuracy.</p> <p>The algorithm identifies and adjusts predictions in three steps:</p> <ol> <li>Critical Region Identification: Define a margin around the decision threshold (e.g., predicted probabilities between 0.4 and 0.6) where predictions are uncertain</li> <li>Favorable Adjustment: For unprivileged group members in the critical region, flip predictions to favorable outcomes (0 \u2192 1)</li> <li>Unfavorable Adjustment: For privileged group members in the critical region, flip predictions to unfavorable outcomes (1 \u2192 0)</li> </ol> <p>By selectively adjusting predictions only for uncertain cases, the method achieves statistical parity while minimizing accuracy loss. The critical region width controls the fairness-accuracy tradeoff:</p> <ul> <li>Wider margins: Greater fairness improvements but more prediction changes</li> <li>Narrower margins: Smaller fairness gains but higher accuracy preservation</li> </ul> <p>This targeted approach differs from threshold-based methods by focusing corrections where the classifier already exhibits uncertainty, making fairness adjustments less disruptive to overall predictive performance.</p> <p>Results:</p> <p></p> <p>Reject Option Classification achieves moderate fairness improvements with severe accuracy degradation:</p> <p>Performance Metrics:</p> Performance Metric Baseline Reject Option Classification Balanced Accuracy 0.74 0.54 F1-Score 0.81 0.77 ROC-AUC 0.83 0.82 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline Reject Option Classification Improvement Debtor Statistical Parity Difference 0.49 0.30 \u2713 Disparate Impact Ratio 3.14 1.43 \u2713 Equal Opportunity Difference 0.24 0.08 \u2713 Average Odds Difference 0.27 0.22 \u2713 Gender Statistical Parity Difference 0.39 0.08 \u2713 Disparate Impact Ratio 1.99 1.08 \u2713 Equal Opportunity Difference 0.31 0.01 \u2713 Average Odds Difference 0.30 0.06 \u2713 Scholarship holder Statistical Parity Difference -0.40 -0.05 \u2713 Disparate Impact Ratio 0.58 0.95 \u2713 Equal Opportunity Difference -0.21 -0.00 \u2713 Average Odds Difference -0.34 -0.05 \u2713 <p>Reject Option Classification achieves comprehensive fairness improvements across all 12 metrics while maintaining strong ROC-AUC (0.82, only 1% below baseline). The method dramatically reduces bias for all protected attributes.</p> <p>However, the 27% drop in Balanced Accuracy (from 0.74 to 0.54) represents a critical weakness. </p> <p>Despite achieving the most uniform fairness improvements among post-processing methods, the substantial accuracy sacrifice makes this approach impractical for deployment scenarios where balanced predictive performance is essential.</p>"},{"location":"bias_mitigation/pre_processing_models/","title":"Pre-Processing Bias Mitigation","text":"<p>Pre-processing techniques transform training data to reduce bias before model training begins. By addressing discriminatory patterns in the data itself, these methods enable any downstream classifier to learn from more equitable representations, without requiring modifications to training algorithms.</p>"},{"location":"bias_mitigation/pre_processing_models/#learning-fair-representations-lfr","title":"Learning Fair Representations (LFR)","text":"<p>R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \u201cLearning Fair  Representations.\u201d International Conference on Machine Learning, 2013</p> <p>Learning Fair Representations transforms input features into an intermediate representation that preserves predictive information while obscuring protected attribute membership. This learned embedding space balances three competing objectives through joint optimization:</p> <ul> <li>Group Fairness: Minimize statistical dependence between the learned representation and protected attributes, preventing the model from encoding demographic information</li> <li>Individual Fairness: Preserve similarity relationships\u2014individuals who are similar in the original feature space remain close in the transformed space, ensuring consistent treatment</li> <li>Predictive Accuracy: Maintain sufficient information in the representation to accurately predict the target outcome</li> </ul> <p>LFR formulates fairness as a constrained optimization problem, learning a probabilistic mapping from original features to a lower-dimensional fair representation. The algorithm uses an encoder-decoder architecture that balances reconstruction accuracy, target prediction capability, and demographic parity simultaneously. This approach provides a theoretically grounded method for removing bias while controlling the fairness-accuracy tradeoff.</p> <p>Results:</p> <p></p> <p>The LFR algorithm demonstrates effective bias reduction with minimal accuracy loss compared to the baseline XGBoost model:</p> <p>Performance Metrics:</p> Performance Metric Baseline LFR Balanced Accuracy 0.74 0.70 F1-Score 0.81 0.79 ROC-AUC 0.83 0.79 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline LFR Improvement Debtor Statistical Parity Difference 0.49 0.45 \u2713 Disparate Impact Ratio 3.14 2.52 \u2713 Equal Opportunity Difference 0.24 0.20 \u2713 Average Odds Difference 0.27 0.25 \u2713 Gender Statistical Parity Difference 0.39 0.43 \u2717 Disparate Impact Ratio 1.99 2.09 \u2717 Equal Opportunity Difference 0.31 0.30 \u2713 Average Odds Difference 0.30 0.36 \u2717 Scholarship holder Statistical Parity Difference -0.40 -0.37 \u2713 Disparate Impact Ratio 0.58 0.62 \u2713 Equal Opportunity Difference -0.21 -0.20 \u2713 Average Odds Difference -0.34 -0.34 - <p>LFR achieves mixed fairness results while preserving strong predictive performance (ROC-AUC: 0.79, only 5% below baseline). </p> <p>The method successfully improves fairness metrics results for Debtor (all metrics improved) and Scholarship holder (3 of 4 metrics improved). However, Gender fairness worsens in 3 metrics, particularly for Statistical Parity Difference and Disparate Impact Ratio. The mixed results highlight the fundamental challenge of simultaneously optimizing multiple fairness criteria\u2014improving group fairness for some attributes may degrade it for others. </p> <p>This tradeoff demonstrates that LFR achieves partial debiasing rather than universal fairness across all protected groups and metrics.</p>"},{"location":"bias_mitigation/pre_processing_models/#reweighing","title":"Reweighing","text":"<p>F. Kamiran and T. Calders, \u201cData Preprocessing Techniques for Classification  without Discrimination,\u201d Knowledge and Information Systems, 2012 </p> <p>Reweighing is a pre-processing technique that assigns sample-specific weights to correct for imbalances in the joint distribution of protected attributes and outcomes. Rather than modifying features or labels, this method adjusts the relative importance of training instances to achieve statistical parity.</p> <p>Each training sample receives a weight calculated based on its protected group membership and target class:</p> <p>\\(w_{i} = \\frac{P(\\text{group}) \\times P(\\text{label})}{P(\\text{group}, \\text{label})}\\)</p> <ul> <li>Underrepresented combinations (e.g., unprivileged group with positive outcomes) receive higher weights, amplifying their influence during training</li> <li>Overrepresented combinations (e.g., privileged group with positive outcomes) receive lower weights, reducing their disproportionate impact</li> <li>The weighted dataset satisfies demographic parity: \\(P(\\hat{Y}=1 | A=0) \\approx P(\\hat{Y}=1 | A=1)\\)</li> </ul> <p>After weight computation, any standard classifier can be trained using weighted loss functions or weighted sampling. This simplicity makes reweighing widely applicable across different model architectures without requiring algorithm-specific modifications. However, extreme weights may arise when certain group-label combinations are rare, potentially leading to training instability or overfitting.</p> <p>Results:</p> <p></p> <p>Reweighing achieves substantial fairness improvements while maintaining strong predictive performance:</p> <p>Performance Metrics:</p> Performance Metric Baseline Reweighing Balanced Accuracy 0.74 0.69 F1-Score 0.81 0.79 ROC-AUC 0.83 0.77 <p>Fairness Improvements:</p> Protected Attribute Metric Baseline Reweighing Improvement Debtor Statistical Parity Difference 0.49 0.27 \u2713 Disparate Impact Ratio 3.14 1.56 \u2713 Equal Opportunity Difference 0.24 0.06 \u2713 Average Odds Difference 0.27 0.08 \u2713 Gender Statistical Parity Difference 0.39 0.13 \u2713 Disparate Impact Ratio 1.99 1.21 \u2713 Equal Opportunity Difference 0.31 0.02 \u2713 Average Odds Difference 0.30 0.05 \u2713 Scholarship holder Statistical Parity Difference -0.40 -0.23 \u2713 Disparate Impact Ratio 0.58 0.74 \u2713 Equal Opportunity Difference -0.21 -0.10 \u2713 Average Odds Difference -0.34 -0.15 \u2713 <p>Reweighing demonstrates exceptional fairness improvements across all protected attributes while maintaining strong predictive performance (ROC-AUC: 0.77, only 7% below baseline). </p> <p>The method achieves consistent metric improvements for Debtor, Gender, and Scholarship holder, with particularly dramatic reductions in bias measures. </p> <p>By correcting distributional imbalances through sample weighting, Reweighing effectively balances fairness and accuracy, making it a strong candidate among pre-processing methods for bias mitigation in student retention prediction. While residual disparities persist, particularly for Debtor (DIR=1.56), the consistent improvements across all demographic groups suggest data-level corrections provide a solid foundation for fair predictions.</p>"},{"location":"data_processing/eda/","title":"Exploratory Data Analysis","text":""},{"location":"data_processing/eda/#dataset-overview","title":"Dataset Overview","text":"<p>The dataset analyzed in this study is the Predict Students' Dropout and Academic Success dataset from the UCI Machine Learning Repository. It is provided by the SATDAP program (Capacita\u00e7\u00e3o da Administra\u00e7\u00e3o P\u00fablica, grant POCI-05-5762-FSE-000191, Portugal). Sourced from a higher education institution, it is compiled from multiple databases and covers student records across a range of undergraduate programs, including agronomy, design, education, nursing, journalism, management, social service, and technology fields.</p> <p>This dataset contains information available at the time of student enrollment, such as academic history, demographic attributes, and socio-economic indicators. It also includes academic performance data from the end of the first and second semesters.</p> <p>In total, the dataset comprises 4,424 rows and 37 variables, offering a comprehensive view of student trajectories and outcomes for fairness analysis.</p>"},{"location":"data_processing/eda/#data-preprocessing-steps","title":"Data Preprocessing Steps","text":"<ol> <li>Column Cleaning: Standardize column names, remove leakage-prone columns, and fix naming inconsistencies.</li> <li>Target Filtering: Keep only 'Dropout' and 'Graduate' values in the target column, mapping them to 0 and 1, accordingly.</li> <li>Standard Dataset Creation: Prepare the dataset for fairness analysis using the <code>StandardDataset</code> class:<ul> <li>Remove rows with missing (NA) values to ensure data completeness</li> <li>Create one-hot encoding of categorical variables for model compatibility</li> <li>Map protected attributes to binary privileged (1) / unprivileged (0) values</li> <li>Map target labels to binary favorable (1) / unfavorable (0) values</li> </ul> </li> <li>Outlier Detection: Use boxplots to detect outliers in all numerical features.</li> <li>Feature Distributions: Plot distributions for all variables (numerical and categorical) to understand their spread and balance.</li> </ol>"},{"location":"data_processing/eda/#outlier-analysis","title":"Outlier Analysis","text":"<p>Key Results:</p> <ul> <li>Application mode and Application order show substantial outliers, with some extreme values that may represent uncommon enrollment pathways or data entry issues.</li> <li>Previous qualification and Previous qualification (grade) contain outliers on both ends, indicating students with exceptionally high or low prior academic performance.</li> <li>Course has a few high outliers, likely representing specialized or less common programs.</li> <li>Admission grade displays numerous outliers above 160, suggesting some students achieved exceptionally high entrance scores.</li> <li>Age at enrollment contains many outliers beyond 40, indicating a population of mature or non-traditional students.</li> <li>Mother's and Father's qualifications and occupations show multiple outliers, reflecting diverse socio-economic backgrounds.</li> <li>Nationality, Displaced, Educational special needs, Debtor, and International are binary, so outliers are non-existent.</li> <li>Unemployment rate, Inflation rate, and GDP have few outliers, as these are macroeconomic indicators that typically follow stable distributions.</li> <li>Target variable shows no outliers, confirming it's a clean binary classification task.</li> </ul>"},{"location":"data_processing/eda/#variable-distributions","title":"Variable Distributions","text":"<p>Key Results:</p> <p>Categorical Features:</p> <ul> <li>Marital status: Heavily dominated by one category (single students), with very few married students</li> <li>Gender: Moderate imbalance, with female gender slightly more represented than male.</li> <li>Scholarship holder: Imbalanced, with more students not holding scholarships</li> <li>Debtor and Educational special needs: Highly imbalanced, with most students not being debtors or having special needs</li> <li>Daytime/evening attendance: Strongly skewed toward daytime attendance</li> <li>Displaced and International: Most students are not displaced or international</li> </ul> <p>Numerical Features:</p> <ul> <li>Admission grade: Right-skewed distribution, with most students having moderate to high admission grades (120-160 range)</li> <li>Age at enrollment: Right-skewed, with most students enrolling in their early 20s; some older outliers present</li> <li>Previous qualification (grade): Normal distribution centered around 140-150</li> <li>Application mode and Application order: Concentrated in specific modes/orders, indicating preferred enrollment pathways</li> <li>Course: Multiple peaks suggest different course popularities</li> <li>Unemployment rate, Inflation rate, and GDP: Economic indicators show temporal variation</li> </ul> <p>Target Variable:</p> <ul> <li>Binary distribution with slight imbalance favoring one class (more graduates than dropouts)</li> </ul> <p>Key findings from the exploratory data analysis reveal significant class imbalances in several categorical features (Gender, Debtor, Scholarship holder) and right-skewed distributions in Age and Admission grades. These characteristics necessitate careful consideration in subsequent fairness analysis and bias mitigation strategies.</p>"},{"location":"explainability/explainability/","title":"Explainability","text":"<p>Explainability methods provide insights into how machine learning models make predictions, allowing us to understand which features drive decisions, detect potential biases, and build trust in automated systems. By visualizing and interpreting model behavior, we can ensure transparency and fairness in data-driven applications.</p>"},{"location":"explainability/explainability/#shap-plots","title":"SHAP Plots","text":""},{"location":"explainability/explainability/#shap-feature-importance","title":"SHAP Feature Importance","text":"<p>Shows the average absolute SHAP value for each feature, indicating which features have the greatest impact on model predictions. </p> <ul> <li>From the plot above, we see that Age at enrollment, Scholarship holder, and Debtor are the most influential features in the model, each with a mean SHAP value above 0.4. </li> <li>This means changes in these features have the largest effect on the model's output. Other important features include Course and Gender, while features like Mother's occupation and Application mode have a smaller impact. </li> <li>The Sum of 14 other features indicates that, collectively, less important features still contribute to the model's decisions. High SHAP values suggest strong influence, so these top features should be carefully considered when interpreting or auditing the model's behavior.</li> </ul>"},{"location":"explainability/explainability/#shap-beeswarm","title":"SHAP Beeswarm","text":"<p>Combines feature importance and the effect direction for each sample, showing how each feature influences individual predictions. It shows how each feature affects individual predictions, with each dot representing a sample. The color indicates the feature value (red = high, blue = low). For example, high values of Age at enrollment (red dots) tend to push the model output higher, while low values (blue dots) push it lower. The spread of dots along the x-axis (SHAP value) shows the range of impact for each feature.</p> <p> The beeswarm plot shows how the value of each feature affects the model's prediction in a positive or negative way:</p> <ul> <li>For Age at enrollment, higher values (red dots) tend to increase the prediction, while lower values (blue dots) decrease it.</li> <li>For Scholarship holder, having a scholarship (high value) generally increases the prediction, while not having one decreases it.</li> <li>For Debtor, being a debtor (high value) pushes the prediction higher, while not being a debtor lowers it.</li> <li>For Course and Gender, the effect depends on the specific value, but you can see that certain values (red or blue) consistently push the prediction up or down.</li> <li>Features with dots spread far to the right (positive SHAP value) increase the model output, while those spread to the left (negative SHAP value) decrease it.</li> <li>Most other features have little effect, as their dots are clustered near zero.</li> </ul> <p>In summary, the plot reveals which feature values drive the prediction higher or lower, helping to understand the direction and strength of each feature's impact on the model's decisions.</p>"},{"location":"explainability/explainability/#shap-dependence-plots-top-5-features","title":"SHAP Dependence Plots (Top 5 Features)","text":"<p>Shows the relationship between a feature's value and its SHAP value, revealing interaction effects and non-linearities for the most important features.  These plots show how the value of each top feature affects the model's prediction:</p> <ul> <li>For Age at enrollment, higher values generally lead to lower SHAP values, meaning the model predicts a lower outcome for older students. The effect is strongest for the youngest ages.</li> <li>For Scholarship holder, having a scholarship (value near 1) is associated with higher SHAP values, so the model predicts a higher outcome for scholarship holders.</li> <li>For Debtor, being a debtor (value near 1) is linked to lower SHAP values, indicating a negative effect on the prediction.</li> <li>For Course, the effect varies by course, but some courses are associated with higher or lower predictions depending on their value.</li> <li>For Gender, the model predicts lower outcomes for one gender (value near 1) and higher for the other (value near 0), showing a clear separation in impact.</li> </ul> <p>The color bars show how another feature interacts with the main feature, revealing possible dependencies. Overall, these plots help identify not only the direction of each feature's effect, but also how combinations of features can influence the model's output.</p> <p>SHAP analysis provides a comprehensive view of how individual features and their values influence model predictions. In this study, a small set of features\u2014especially Age at enrollment, Scholarship holder, and Debtor\u2014were found to drive most of the model's decisions, with clear positive or negative effects depending on their values. Other features play a more minor role. The combination of bar, beeswarm, and dependence plots allows us to interpret not only which features matter most, but also how they interact and affect outcomes for different individuals. This level of transparency is essential for building trust, detecting bias, and ensuring fairness in machine learning applications.</p>"}]}